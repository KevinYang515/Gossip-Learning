{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KevinLBJ\\Anaconda3\\envs\\fl\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\KevinLBJ\\Anaconda3\\envs\\fl\\lib\\site-packages\\tensorflow_core\\python\\ops\\distributions\\distribution.py:265: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From C:\\Users\\KevinLBJ\\Anaconda3\\envs\\fl\\lib\\site-packages\\tensorflow_core\\python\\ops\\distributions\\bernoulli.py:169: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-527c290cce95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdivision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;31m# We still need all the names that are toplevel on tensorflow_core\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow_core\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;31m# In V1 API we need to print deprecation messages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\site-packages\\tensorflow_core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf_logging\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_logging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;34m\"\"\"Import the target module and insert it into the parent's namespace.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_module_globals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_local_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\importlib\\__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\site-packages\\tensorflow_core\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# We import dense_features_v2 first so that the V1 DenseFeatures is the default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# if users directly import feature_column_lib.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_features_v2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense_features\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\dense_features_v2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdense_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_v2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\dense_features.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_v2\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mserialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\site-packages\\tensorflow_core\\python\\feature_column\\feature_column_v2.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfc_old\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfc_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fl\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import data.preprocessing as prep\n",
    "from model.model import Device_Model, generate_weight\n",
    "\n",
    "from math import floor\n",
    "from sklearn.utils import shuffle\n",
    "from random import randint\n",
    "from tensorflow.keras import datasets\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 data\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "train_labels = train_labels.reshape(len(train_labels),)\n",
    "train_label = to_categorical(train_labels)\n",
    "test_label = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training_data_selected_random(device_num, device_data_amount):\n",
    "    temp_amount = round(device_data_amount/10)\n",
    "    image, label = train_images, train_labels\n",
    "    \n",
    "    temp_image, temp_label = image[label == 0], label[label == 0]\n",
    "    shuffle(temp_image, temp_label, random_state=device_num)\n",
    "    \n",
    "    s0 = [temp_image[:temp_amount], temp_label[:temp_amount]]\n",
    "    \n",
    "    for classes in range(1, 10):\n",
    "        temp_image, temp_label = image[label == classes], label[label == classes]\n",
    "        shuffle(temp_image, temp_label, random_state=device_num * classes)\n",
    "\n",
    "        s1 = [temp_image[:temp_amount], temp_label[:temp_amount]]\n",
    "        s0 = [np.append(s0[0], s1[0], axis=0), np.append(s0[1], s1[1])]\n",
    "    \n",
    "    s0[0], s0[1] = shuffle(s0[0], s0[1], random_state=randint(0, device_num))\n",
    "\n",
    "    return s0[0], to_categorical(s0[1], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    epoch = _ + epo\n",
    "    # initial_lrate = 1.0 # no longer needed\n",
    "    drop = 0.99\n",
    "    epochs_drop = 1.0\n",
    "    \n",
    "    lrate = 0.2 * pow(drop, floor((1+epoch)/epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust parameters of the model\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(step_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = ImageDataGenerator(preprocessing_function=prep.preprocessing_for_training)\n",
    "history_02 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def define_model():\n",
    "#     return tf.keras.Sequential([\n",
    "#         layers.Conv2D(64, (5, 5), padding='same', activation='relu', input_shape=(24, 24, 3)),\n",
    "#         layers.MaxPool2D(pool_size=3, strides=2, padding='same'),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.Conv2D(64, (5,5), padding='same', activation='relu'),\n",
    "#         layers.MaxPool2D(pool_size=3, strides=2, padding='same'),\n",
    "#         layers.BatchNormalization(),\n",
    "#         layers.Flatten(),\n",
    "#         layers.Dense(384, activation='relu'),\n",
    "#         layers.Dropout(0.5),\n",
    "#         layers.Dense(192, activation='relu'),\n",
    "#         layers.Dropout(0.5),\n",
    "#         layers.Dense(10, activation='softmax')\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training_data_selected_random_range(device_num, data_range):\n",
    "    image, label = train_images, train_labels\n",
    "\n",
    "    if data_range[0] < data_range[1]:\n",
    "        s0 = [image[label == 0][data_range[0]:data_range[1]], label[label == 0][data_range[0]:data_range[1]]]\n",
    "\n",
    "        for classes in range(1, 10):\n",
    "            s1 = [image[label == classes][data_range[0]:data_range[1]], label[label == classes][data_range[0]:data_range[1]]]\n",
    "            s0 = [np.append(s0[0], s1[0], axis=0), np.append(s0[1], s1[1])]\n",
    "\n",
    "    else:\n",
    "        s1 = [image[label == 0][data_range[0]:], label[label == 0][data_range[0]:]]\n",
    "        a = image[label == 0][:data_range[1]]\n",
    "        a_label = label[label == 0][:data_range[1]]\n",
    "\n",
    "        s0 = [np.append(a, s1[0], axis=0), np.append(a_label, s1[1])]\n",
    "\n",
    "        for classes in range(1, 10):\n",
    "            s1 = [image[label == classes][data_range[0]:], label[label == classes][data_range[0]:]]\n",
    "            a = image[label == 0][:data_range[1]]\n",
    "            a_label = label[label == 0][:data_range[1]]\n",
    "            s1 = [np.append(a, s1[0], axis=0), np.append(a_label, s1[1])]\n",
    "\n",
    "            s0 = [np.append(s0[0], s1[0], axis=0), np.append(s0[1], s1[1])]\n",
    "\n",
    "    for _ in range(20):\n",
    "        s0[0], s0[1] = shuffle(s0[0], s0[1], random_state=randint(0, device_num))\n",
    "        \n",
    "    return s0[0], to_categorical(s0[1], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Device_Model():\n",
    "#     def __init__(self, number):\n",
    "#         self.number = number\n",
    "#         self.weight = define_model()\n",
    "#         self.weight.compile(optimizer='sgd',\n",
    "#                                   loss='categorical_crossentropy',\n",
    "#                                   metrics=['accuracy'])\n",
    "#         self.weight.load_weights('ini_cifar10.h5')\n",
    "        \n",
    "#         self.client = {}\n",
    "#         self.history = {}\n",
    "#         self.first = {}\n",
    "#         self.master_first = True\n",
    "#         self.weight_hat = define_model()\n",
    "#         self.weight_q = define_model()\n",
    "        \n",
    "#         self.weight_list = []\n",
    "        \n",
    "#     def add_client(self, client, degree):\n",
    "#         if client not in self.client:\n",
    "#             self.client[client] = {}\n",
    "#             self.client[client]['weight'] = define_model()\n",
    "#             self.client[client]['degree'] = degree\n",
    "#             self.first[client] = True\n",
    "#         else:\n",
    "#             print(client, \"has already existed in client dictionary.\")\n",
    "        \n",
    "#     def add_client_list(self, client, client_dictionary):\n",
    "#         for cli in client:\n",
    "#             if cli not in self.client:\n",
    "#                 self.client[cli] = {}\n",
    "#                 self.client[cli]['weight'] = define_model()\n",
    "#                 self.client[cli]['degree'] = len(client_dictionary[cli])\n",
    "#                 self.first[cli] = True\n",
    "#             else:\n",
    "#                 print(cli, \"has already existed in client dictionary.\")\n",
    "        \n",
    "#     def set_client_weight(self, client, value):\n",
    "#         if client in self.client:\n",
    "#             if self.first[client] == True:\n",
    "#                 self.first[client] = False\n",
    "#                 self.client[client]['weight'].set_weights(value.get_weights())\n",
    "#             else:\n",
    "#                 for layer in range(len(self.weight.layers)):\n",
    "#                     self.client[client]['weight'].layers[layer].set_weights(\n",
    "#                         np.add(value.layers[layer].get_weights(),\n",
    "#                           self.client[client]['weight'].layers[layer].get_weights()))\n",
    "#         else:\n",
    "#             print(client, \"doesn't exist in client dictionary.\")\n",
    "    \n",
    "#     def update_own_q(self):\n",
    "#         if self.master_first == True:\n",
    "#             self.master_first = False\n",
    "#             self.weight_hat.set_weights(self.weight_q.get_weights())\n",
    "#         else:\n",
    "#             for layer in range(len(self.weight.layers)):\n",
    "#                 self.weight_hat.layers[layer].set_weights(\n",
    "#                     np.add(self.weight_q.layers[layer].get_weights(), \n",
    "#                       self.weight_hat.layers[layer].get_weights()))\n",
    "            \n",
    "#     def count_q(self):\n",
    "#         if self.master_first == True:\n",
    "#             self.weight_q.set_weights(self.weight.get_weights())\n",
    "#         else:\n",
    "#             for layer in range(len(self.weight.layers)):\n",
    "#                 self.weight_q.layers[layer].set_weights(\n",
    "#                     np.subtract(self.weight.layers[layer].get_weights(), \n",
    "#                     self.weight_hat.layers[layer].get_weights()))\n",
    "    \n",
    "#     def update_parameter(self):\n",
    "#         if self.master_first == False:\n",
    "#             client_list = [x for x in self.client]\n",
    "            \n",
    "#             temp = {}\n",
    "            \n",
    "#             for device in client_list:\n",
    "#                 client_weight = self.weight_list[device] * 0.5\n",
    "                \n",
    "#                 if client_list.index(device) == 0:\n",
    "#                     for layer in range(len(self.weight.layers)):\n",
    "#                         temp[layer] = np.subtract(self.client[device]['weight'].layers[layer].get_weights(), \n",
    "#                                     self.weight_hat.layers[layer].get_weights())\n",
    "#                 elif client_list.index(device) == len(client_list) - 1:\n",
    "#                     for layer in range(len(self.weight.layers)):\n",
    "#                         self.weight.layers[layer].set_weights(\n",
    "#                             np.add(self.weight.layers[layer].get_weights(),\n",
    "#                               np.multiply(np.add(temp[layer], \n",
    "#                                   np.subtract(self.client[device]['weight'].layers[layer].get_weights(), \n",
    "#                                     self.weight_hat.layers[layer].get_weights())), client_weight)))\n",
    "                                \n",
    "#                 else:\n",
    "#                     for layer in range(len(self.weight.layers)):\n",
    "#                         temp[layer] = np.add(temp[layer], \n",
    "#                                         np.subtract(self.client[device]['weight'].layers[layer].get_weights(), \n",
    "#                                           self.weight_hat.layers[layer].get_weights()))\n",
    "                        \n",
    "#     def set_weight(self, device_dictionary):\n",
    "#         self.weight_list = generate_weight(self.number, device_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dictionary(dic):\n",
    "    temp = sorted(dic.keys())\n",
    "    temp_dic = {}\n",
    "    \n",
    "    for key in temp:\n",
    "        temp_dic[key] = dic[key]\n",
    "    \n",
    "    return temp_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_weight(node, device_client_dic):\n",
    "#     temp = []\n",
    "    \n",
    "#     for j in range(num_node):\n",
    "#         if j in device_client_dic[node]:\n",
    "#             temp.append(1/(max(len(device_client_dic[node]), len(device_client_dic[j])) + 1))\n",
    "#         else:\n",
    "#             temp.append(0)\n",
    "            \n",
    "#     temp_v = 0    \n",
    "#     for j in device_client_dic[node]:\n",
    "#         temp_v += temp[j]\n",
    "        \n",
    "#     temp[node] = 1 - temp_v\n",
    "    \n",
    "#     return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_client_dic = {}\n",
    "alg_selected_device = []\n",
    "\n",
    "total_demand = 50000\n",
    "num_node = 15\n",
    "\n",
    "for n in range(num_node):\n",
    "    node_list = [x for x in range(num_node)]\n",
    "\n",
    "    node_list.remove(n)\n",
    "\n",
    "    device_client_dic[n] = node_list\n",
    "\n",
    "device_client_list = [x for x in device_client_dic]\n",
    "        \n",
    "for num in range(num_node):\n",
    "    alg_selected_device.append(total_demand/num_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_device = len(alg_selected_device)\n",
    "num_center_epoch = 1\n",
    "num_local_epoch = 5\n",
    "num_round = 10000\n",
    "center_batch_size = 64\n",
    "local_batch_size = 64\n",
    "\n",
    "show = 0\n",
    "\n",
    "device_train_data = {}\n",
    "device_test_data = {}\n",
    "\n",
    "for _ in range(num_round):\n",
    "    print(\"\\033[1m\" + \"Round: \" + str(_) + '\\033[0m')\n",
    "    start_with = 0\n",
    "    end_with = 0\n",
    "\n",
    "    for device in device_client_dic:\n",
    "        if(_ == 0):\n",
    "            #Define an estimator model\n",
    "            #Initialize every device (e.g., all devices are initialized with same parameters)\n",
    "            locals()['model_{}'.format(device)] = Device_Model(device)\n",
    "            locals()['model_{}'.format(device)].history['val_loss'] = [2.3840]\n",
    "            locals()['model_{}'.format(device)].history['val_acc'] = [0.0976]\n",
    "\n",
    "            locals()['model_{}'.format(device)].add_client_list(device_client_dic[device], device_client_dic)\n",
    "            locals()['model_{}'.format(device)].set_weight(device_client_dic, num_node)\n",
    "\n",
    "            temp_arange = []\n",
    "            temp_amount = int(alg_selected_device[device]/10) + 10\n",
    "\n",
    "            end_with += temp_amount\n",
    "            end_with %= 5000\n",
    "            temp_arange = [start_with, end_with]\n",
    "\n",
    "            train_image_temp, train_label_temp = prepare_for_training_data_selected_random_range(device, temp_arange)\n",
    "            start_with = end_with\n",
    "            start_with %= 5000\n",
    "\n",
    "            device_train_data[device] = [train_image_temp, train_label_temp]\n",
    "\n",
    "\n",
    "        #Local training on each device \n",
    "        for epo in range(num_local_epoch):\n",
    "            train_image_crop = np.stack([prep.random_crop(device_train_data[device][0][i], 24, 24) for i in range(len(device_train_data[device][0]))], axis=0)\n",
    "\n",
    "            for random_ in range(10):\n",
    "                train_new_image, train_new_label = shuffle(train_image_crop, \n",
    "                                                           device_train_data[device][1], \n",
    "                                                           random_state=randint(0, train_image_crop.shape[0]))\n",
    "\n",
    "            history_temp = locals()['model_{}'.format(device)].weight.fit_generator(\n",
    "                augment.flow(train_new_image, train_new_label, batch_size=local_batch_size), \n",
    "                epochs=1, \n",
    "                callbacks=[callback],\n",
    "                verbose=show)\n",
    "\n",
    "        # Update from x^(t+1/2) to x^(t+1)   (line 4)\n",
    "        locals()['model_{}'.format(device)].update_parameter()\n",
    "\n",
    "        # Count weight_q (line 5)\n",
    "        locals()['model_{}'.format(device)].count_q()\n",
    "\n",
    "    # Update all weight_hat for each device (line 6, 7)\n",
    "    for device in device_client_dic:\n",
    "        for dev in device_client_dic[device]:\n",
    "            locals()['model_{}'.format(device)].set_client_weight(dev, locals()['model_{}'.format(dev)].weight_q)\n",
    "\n",
    "    # Update own weight_hat for each device (line 8)\n",
    "    for device in device_client_dic:\n",
    "        locals()['model_{}'.format(device)].update_own_q()\n",
    "\n",
    "    for device in device_client_dic:\n",
    "        #Evaluate with new weight\n",
    "        test_d = np.stack([prep.preprocessing_for_testing(test_images[i]) for i in range(10000)], axis=0)\n",
    "\n",
    "        test_new_image, test_new_label = shuffle(test_d, test_label, \n",
    "                                                 random_state=randint(1, train_images.shape[0]))\n",
    "\n",
    "        history_temp = locals()['model_{}'.format(device)].weight.evaluate(test_new_image, \n",
    "                                                                           test_new_label, \n",
    "                                                                           batch_size=64,\n",
    "                                                                           verbose=show)\n",
    "\n",
    "        #Record each round accuracy and loss for every device\n",
    "        locals()['model_{}'.format(device)].history['val_loss'].append(history_temp[0])\n",
    "        locals()['model_{}'.format(device)].history['val_acc'].append(history_temp[1])\n",
    "        print(str(_), device, \" Result: \", str(history_temp[1]))\n",
    "\n",
    "#     if _ % 100 == 0 or _ == num_round - 1:\n",
    "#         with open('Federated_Learning_Data/15-nodes/fully_result_matrix_05.txt', 'w+') as f:\n",
    "#             for device in device_client_dic:\n",
    "#                 f.write(str(device))\n",
    "#                 f.write(str(locals()['model_{}'.format(device)].history['val_acc']))\n",
    "#                 f.write(str(locals()['model_{}'.format(device)].history['val_loss']))\n",
    "#                 f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
